\include{commons}
\lecturetitle{Lecture 17: Primality testing (2)} 

\begin{frame}\frametitle{Efficient algorithms}
  This lecture proves that the two forms of the Fermat's Little
  Theorem are equivalent.  

  We then outline a primality testing algorithm based on the Fermat's
  Little Theorem.  This algorithm requires two subroutines for
  \begin{itemize}
  \item computing powers $a^n$,
  \item finding the greatest common divisors.
  \end{itemize}
  We will present efficient algorithms for these two problems.
\end{frame}

\begin{frame}\frametitle{Fermat's Little Theorem}
  The common form of the Fermat's Little Theorem is as follows.

  \begin{tcolorbox}[title=First form]
    \textcolor{blue}{Theorem:} If $p$ is a prime and $a$ is an integer not divisible by $p$, then $a^{p-1}\Mod p = 1$.
  \end{tcolorbox}

  However, we prove the following version in the previous lecture:

  \begin{tcolorbox}[title=Second form]
    \textcolor{blue}{Theorem:} If $p$ is a prime, then $p|a^{p}-a$, for any integer $a$.
  \end{tcolorbox}

  We shall prove that both forms are equivalent.
\end{frame}

\begin{frame}\frametitle{Basic claim}
  We need the following claim to prove the equivalence.

  \begin{tcolorbox}
    {\bf Claim 1:} If $p$ is a prime and $p| ab$ but $p\not| a$, then
    $p|b$.
  \end{tcolorbox}
  
  {\bf Proof:} Factor $ab$ into a product of primes.  Since $p$
  divides $ab$ then $p$ must be one of the factors in the prime
  factorization of $ab$.  However, since $p\not|a$, $p$ does not
  appear in $a$'s prime factorization.  This implies that $p$ must be
  from $b$'s prime factorization.  Hence, $p|b$.  $\blacksquare$

  \vspace{0.2in}

  Note that the assumption that $p$ is a prime is crucial.  For
  example, if we let $p=6$, $a=9$, and $b=2$, we can see that $6|18$,
  but $6\not|9$ or $6\not|2$.
\end{frame}

\begin{frame}\frametitle{Another form for the Fermat's Little Theorem}
  To show that both forms of the Fermat's Little Theorem are
  equivalent, we need to prove the following two claims.  (You should
  look at the claims and try to understand why proving both of them
  implies that both forms of the Fermat's Little Theorem are
  equivalent.)

  \begin{tcolorbox}[title=Second form $\Rightarrow$ first form]
    \textcolor{blue}{Claim 2:} If $p$ is a prime, $p$ does not divide an integer $a$, and $p|a^p-a$; then $a^{p-1}\Mod p = 1$.
  \end{tcolorbox}

  {\bf Proof:} Write $a^p-a = a(a^{p-1}-1)$.  Since $p|a(a^{p-1}-1)$
  but $p\not|a$, from claim 1, we have that $p|a^{p-1}-1$.  Thus, 
  \[ a^{p-1}\Mod p = 1, \]
  as required. $\blacksquare$
\end{frame}

\begin{frame} 
  \begin{tcolorbox}[title=First form $\Rightarrow$ second form]
    \textcolor{blue}{Claim 3:} Lat $p$ be a prime.  If for any integer
    $a$ not divisible by $p$, $a^{p-1}\Mod p = 1$, then for any
    integer $b$, we have that $p|b^p-b$.
  \end{tcolorbox}
  
  {\bf Proof:} There are two cases, when $p\not|b$ and $p|b$.  

  In the first case where $p\not|b$, the assumption implies that
  $p|b^{p-1}-1$; thus, $p|b(b^{p-1}-1)$, as required.

  Now consider the case when $p|b$.  Since we can write
  $b^p-b=b(b^{p-1}-1)$, we know that $p|b(b^{p-1}-1)$, because
  $p|b$. $\blacksquare$
\end{frame}

\begin{frame}\frametitle{The test: idea}
  If we want to check if $q$ is a prime, from the Fermat's Little
  Theorem, we know that when $q$ is a prime, for any integer $a$ not
  divisible by $q$, $a^{q-1}\Mod q=1$.  We can devise a test based on
  this fact:

  \begin{tcolorbox}
    {\bf PROCEDURE} FermatTest0($q$,$a$)\\
    1. Find $y = a^{q-1}\Mod q$\\
    2. {\bf if} $y\neq 1$ {\bf then}\\
    3. \ \ {\bf return} ``COMPOSITE''\\
    4. {\bf else}\\
    5. \ \ {\bf return} ``PRIME''\\
    6. {\bf endif}
  \end{tcolorbox}

  To implement this test efficiently, we need a way to find
  $a^{q-1}\Mod q$ quickly.  Recall that in our test, $q$ will be a
  very large number and an $O(q)$-time algorithm or even
  $O(\sqrt{q})$-time algorithm will not be fast enough.
\end{frame}

\begin{frame}\frametitle{Computing powers}
  If we want to find $a^n\Mod q$, a straight-forward implementation is
  to perform $n$ multiplications.  When $a$ and $q$ are $m$-bit
  numbers, this takes $O(nm^2)$ time because multiplication and
  division run in $O(m^2)$ time.

  Can we compute the power faster?

  Let's start with some example.
  \begin{itemize}
  \item How to compute $a^2$?  We can't do anything faster then just
    computing directly $a^2$.
  \item What can we compute if we know $a^2$?  Note that we can square
    $a^2$ to get $a^2\times a^2=a^4$.
  \item We can keep squaring to get $a^8=a^4\times a^4$,
    $a^{16}=a^8\times a^8$, and so on.
  \item Thus, we can compute $a^{2^k}$ with only $k$ multiplications.
  \end{itemize}

  We know how to compute the power $a^n$ when $n$ is a power of $2$.
  We still need to handle the case when the exponent $n$ is not a
  power of $2$.
\end{frame}

\begin{frame}
  To deal with general case, we can either keep all computed powers
  and multiply appropriate results to get the final answer.  For
  example, to compute $a^{20}$, we note that $a^{20}=a^{16}\cdot a^4$,
  so we first compute $a^2,a^4,a^8,a^{16}$ and multiply $a^{16}$ and
  $a^4$.

  \vspace{0.1in} A recursive algorithm below takes another approach.
  It first finds $a^{\lfloor n/2\rfloor}$, where $\lfloor x\rfloor$ is
  a floor function that returns the largest integer not greater than
  $x$ (e.g., $\lfloor 1.2\rfloor=1$ and $\lfloor 2\rfloor=2$).  It
  then squares the result and depending on the value of $n/2$,
  multiplies the result with $a$.

  \begin{tcolorbox}
    {\bf PROCEDURE} Power($a$,$n$)\\
    1. {\bf if} $n=1$ {\bf then} {\bf return} $a$ {\bf endif}\\
    2. {\bf let} $b\leftarrow$ Power($a$,$\lfloor n/2\rfloor$)\\
    3. {\bf if} $2|n$ {\bf then} \\
    4. \ \ {\bf return} $b\times b$ \\
    5. {\bf else}\\
    6. \ \ {\bf return} $b\times b\times a$ \\
    7. {\bf endif}
  \end{tcolorbox}

\end{frame}

\begin{frame}\frametitle{The number of multiplications}
  Let's analyze the number of multiplications required to compute
  $a^n$.  We will not be very precise here, and you will learn more
  about this techniques in later courses.

  First, we can see that multiplications appear on line 4 and 6.
  Excluding the multiplications excuted in the recursive calls, each
  invocation of procedure Power requires at most 3 multiplications.
  To analyze the number of multiplications, we only need to look at
  the depth of the recursion.

  Let's look at an example of Power(a,140):

  \begin{tcolorbox} 
    Power(a,135) $\rightharpoonup$ Power(a,67) $\rightharpoonup$
    Power(a,33) $\rightharpoonup$ Power(a,16) $\rightharpoonup$
    Power(a,8) $\rightharpoonup$ Power(a,4) $\rightharpoonup$
    Power(a,2) $\rightharpoonup$ Power(a,1)
  \end{tcolorbox}       
  
  One important observation is that each recursive call decreases $n$
  by at least a factor of 2.
\end{frame}

\begin{frame}
  Since for each level of recursion, $n$ decrease by at least a factor
  of $2$.  Let $n_i$ be the value of variable $n$ after $i$ levels of
  recursion.  Clearly, $n_0=n$.  We have that $n_1\leq n_0/2$,
  $n_2\leq n_1/2$, and so on, or in general,
  \[ n_{i+1}\leq n_i/2. \]
  This implies that
  \[ n_i \leq n_0/2^i = n/2^i. \]

  Also, note that the algorithm stops when $n=1$.  Let $k$ be the
  maximum level of recursion.  We then have this equation
  \[ 1 = n_k \leq n/2^k, \]
  or $2^k\leq n$. To solve for $k$, we can use logarithms.  Taking
  logarithms on both side, we get that
  \[ k \leq \log n, \]
  where $\log(\cdot)$ is a logarithm based 2.  Thus, the number of
  multiplications required is $O(\log n)$.
\end{frame}
 
\begin{frame}\frametitle{The running time}
  We have shown that the number of multiplications is $O(\log n)$.
  However, we treat all multiplications as a unit of computation.
  While this is OK, when the numbers are small enough.  However, in
  our case where we deal with large numbers (e.g., 1000-bit numbers),
  we need to consider the size of the integers when multiplying
  them.

  \vspace{0.08in}
  Let $a$ and $n$ be $m$-bit integers. The number of bits for $a^n$ is
  $nm$, which is extremely large.  If we need the actual number, there
  is no way to find it efficiently, as the number of bits in the
  output is already exponential in $m$.  (Recall that $n$ can be as
  large as $2^m$.)

  \vspace{0.08in}
  But in our case, we do not want the actual number $a^n$, but
  $a^n\Mod q$, where $q$ is an $m$-bit number.  Therefore, we can keep
  all numbers below $q$ by performing a modulo operation after every
  multiplication.  This means that each multiplication runs in
  $O(m^2)$ time.  Therefore, the algorithm runs in $O(m^2\log n)$.
  Since $n$ is an $m$-bit integer, it is at most $2^m$, implying $\log
  n\leq m$.  The running time for Power is $O(m^3)$.
\end{frame}

\begin{frame}\frametitle{The greatest common divisors}
  Another important concept in number theory is the greatest common
  divisor.

  A {\bf common divisor} for integers $a$ and $b$ is an integer $c$
  such that $c|a$ and $c|b$.  A common divisor $c$ is {\bf the
    greatest common divisor} if, for every common divisor $c'$,
  $c'\leq c$.  We denote the greatest common divisor of $a$ and $b$ as
  $gcd(a,b)$.

  \vspace{0.1in}
  \begin{tcolorbox}[title=Examples]
    \[ gcd(100,15) = 5. \] 
    \[ gcd(7,15) = 1. \] 
    \[ gcd(1000,120) = 40. \] 
    \[ gcd(2558,2530) = 2. \] 
  \end{tcolorbox}
\end{frame}

\begin{frame}\frametitle{Finding the greatest common divisors}
  \begin{itemize}
  \item The most straight-forward algorithm for finding the gcd of two
    integers $a$ and $b$ is to iteratively divide both numbers by
    integers from $1$ to $\min\{a,b\}$.
  \item However, this will not be a polynomial time algorithm.
  \item We may want to use the trick from the prime testing algorithm
    where we only divide the numbers up to their square roots, i.e.,
    we test for numbers from $1$ to $\sqrt{\min\{a,b\}}$.
  \item But this does not work. {\em Can you find an example where this
    approach is broken?}
  \end{itemize}
\end{frame}

\begin{frame}\frametitle{The Euclidean algorithm}
  The following Euclidean algorithm finds the $gcd$ of $a$ and $b$ when
  both integers are non-negative.

  \vspace{0.1in}
  \begin{tcolorbox}[title=The Euclidean algorithm]
    {\bf PROCEDURE} GCD($a$,$b$)\\
    1. {\bf if} $a\Mod b=0$ {\bf then} \\
    2. \ \ {\bf return} $b$ \\
    3. {\bf else} \\
    4. \ \ {\bf return} GCD($b$, $a\Mod b$)\\
    5. {\bf endif}
  \end{tcolorbox}

  \vspace{0.2in} Note that we use GCD($a$,$b$) to denote the output of
  this algorithm but we use $gcd(a,b)$ (small caps/math fonts) to
  denote the {\bf correct} greatest common divisor.
\end{frame}

\begin{frame}\frametitle{An example}
  Suppose we want to find $gcd(12,81)$.  The execution of GCD(12,81)
  works as follows:

  \vspace{0.2in}
  \begin{tabular}{l|c|c|c|c|c}
    & $a$ & $b$ & $b|a$? & $a\Mod b$ & result \\
    \hline
    GCD(15,81) & 15 & 81 & no & 15 & - \\
    GCD(81,15) & 81 & 15 & no & 6 & - \\
    GCD(15,6) & 15, & 6 & no & 3 & - \\
    GCD(6,3) & 6 & 3 & yes & - & 3 \\
    \hline
  \end{tabular}
\end{frame}

\begin{frame}\frametitle{Why does this work?}
  First, let's try to understand why this algorithm works.

  \begin{itemize}
    \item The first case is clear: since the largest divisor for $b$
      is $b$ itself, when $b|a$, $b$ must be the $gcd$.
    \item Now, suppose that $b\not|a$.  Suppose that $c$ is a common
      divisor.  Since $c$ divides $b$, it must divide any multiple of
      $b$, i.e. $c|kb$ for any integer $k$. Note also that we can
      write
      \[ a\Mod b = a - kb, \]
      for some integer $k$.  Therefore, if $c$ divides both $a$ and
      $b$, it must also divide $a\Mod b$.
    \item On the other hand, if an integer $c$ divides both $b$ and
      $a\Mod b$. We can show that $c$ divides $a$, because we can
      write
      \[ a = kb + (a\Mod b), \]
      for some integer $k$.
  \end{itemize}
\end{frame}

\begin{frame}
  \begin{itemize}
  \item
    From the previous discussion, we know that the set of common
    divisors of $a$ and $b$, and of $b$ and $a\Mod b$ are the same.
  \item
    Therefore, the maximum of these divisors must be the same, i.e.,
    \[ gcd(a,b) = gcd(b,a\Mod b). \]
  \item This explains why the``else'' part is correct.
  \item To get a formal proof of the algorithm's correctness, we need
    mathematical induction.  We leave this as an exercise.
  \end{itemize}
\end{frame}

\begin{frame}\frametitle{The running time}
  \begin{itemize}
  \item To prove the running time, we must be able to argue about the
    progress the algorithm makes.
  \item Before we start, let's make an assumption that $a\geq b$.
    \begin{itemize}
    \item
      Note that this is always true, except in the first step.  But
      after the first step this is always true since $a\Mod b < b$.
    \item
      Therefore, if $a<b$, we spend one iteration of GCD and then our
      assumption is always true.
    \end{itemize}
  \item This also ensure that the values of variable $b$ are
    decreasing as $a\Mod b$ is always smaller than $b$.  This shows
    that the algorithm terminates in at most $b$ steps ({\em why?}),
    but it is not strong enough to give us a good bound on the running
    time.
  \end{itemize}
\end{frame}

\begin{frame}

  Let's try to look at how the values of parameters $a$ and $b$ chan
  ges over time.  Let $a_i$ and $b_i$ be the values of variables $a$
  and $b$ at the $i$-th iteration.  (Note that $a=a_1$ and $b=b_1$.)
  The following table shows a few values of $a_i$'s and $b_i$'s. 
  
  \vspace{0.2in}
  \begin{tabular}{c|c|c}
    $i$ & $a_i$ & $b_i$\\ \hline
    1 & $a$ & $b$ \\
    2 & $b$ & $a\Mod b$ \\
    3 & $a\Mod b$ & $b\Mod (a\Mod b)$ \\
  \end{tabular}
  \vspace{0.2in}
  
  Note that the values of variable $a$ change from $a$ to $b$ and to
  $a\Mod b$.  If we want a polynomial running time (in the number of
  bits), we want $a$ or $b$ to decrese very quickly.
\end{frame}

\begin{frame}
  Let's try to focus on $a$.  Can we say that $a$ always decreases by
  a factor of 2 in every iteration?
    
  \begin{itemize}
  \item Probably not.  But we know that, if $b\leq a/2$, this is true,
    i.e., $a_{i+1}=b_i\leq a_i/2$. 
  \item How about the case when $b>a/2$ ? Can we say anything in this
    case? \pause
  \item Clearly, in iteration $i+1$, $a_{i+1}=b_i$, which can be
    larger than $a_i$.  But let's look at the next iteration $i+2$.
    Now $a_{i+2}=b_{i+1}=a_i\Mod b_i$.  Note that since $b_i>a_i/2$,
    $a_i\Mod b_i = a_i - b_i < a_i/2$.  Thus, we can conclude that
    $a_{i+2}\leq a_i/2$.
  \end{itemize}

  Therefore, in any case, we know that after at least two iterations,
  $a$ decreases by at least a factor of 2.  Clearly the algorithm
  stops when $a\leq 1$.  Therefore, the number of iterations is at
  most the number of times we can divide $a$ by 2 until $a$ is no
  larger than 1.  This is at most $\log a$.
\end{frame}

\begin{frame}
  If we consider the case where $b>a$, we have to account for the fact
  that we swap $a$ and $b$.  The number of iterations is $O(\max\{\log
  a,\log b\})$, or equivalently $O(\log a + \log b)$.

  If $a$ and $b$ are $m$-bit numbers, each iteration runs in time
  $O(m^2)$ (the time for a modulo operation).  Thus the algorithm runs
  in time
  \[
  O((\log a + \log b)m^2)=O(m^3),
  \]
  because $\log a \leq m$ and $\log b\leq m$.
\end{frame}

\begin{frame}\frametitle{The final Fermat test: idea + more condition}
  We can incorporate the $gcd$ computation in our primality testing
  algorithm.
  
  If $q$ is a prime and $a$ is chosen so that $2\leq a\leq q-1$, we
  know that $gcd(q,a)=1$.  We shall add this condition to the test as
  well.  While this check might not be that helpful in itself when $q$
  is composite, it is crucial to our proof that the algorithm works
  (described in the next lecture).

  \begin{tcolorbox}
    {\bf PROCEDURE} FermatTest($q$,$a$)\\
    1. {\bf if} $GCD(q,a)\neq 1$ {\bf then} {\bf return} ``COMPOSITE'' {\bf endif}\\
    2. Find $y = a^{q-1}\Mod q$\\
    3. {\bf if} $y\neq 1$ {\bf then}\\
    4. \ \ {\bf return} ``COMPOSITE''\\
    5. {\bf else}\\
    6. \ \ {\bf return} ``PRIME''\\
    7. {\bf endif}
  \end{tcolorbox}
\end{frame}

